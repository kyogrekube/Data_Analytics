#means they have a higher vaue, rarer
par(mar=rep(0.2,4))
heatmap(dataMatrix)
hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order,]
par(mfrow = c(1,3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(rowMeans(dataMatrixOrdered), 40:1 , xlab = "The Row Mean", ylab="Row", pch=19)
plot(rowMeans(dataMatrixOrdered), xlab = "Column", ylab="Column Mean", pch=19)
#Exercise 2
abalone <- read.csv(url("https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data"), header = FALSE, sep=",")
#add column names
colnames(abalone) <- c("sex", "length", "diameter", "height", "whole_weight", "shucked_weight", "viscera_weight", "shell_weight", "rings")
#EDA
View(abalone)
summary(abalone)
str(abalone)
summary(abalone$rings)
abalone$rings <- as.numeric(abalone$rings)
abalone$rings <- cut(abalone$rings, br=c(-1, 8, 11, 35), labels = c("young", "adult", "old"))
abalone$rings <- as.factor(abalone$rings)
summary(abalone$rings)
#sex variable is not numeric, KNN needs numeric values, so let's get rid of it
aba <- abalone
aba$sex <- NULL
#normalize dataset
normalize <- function(x){
return((x - min(x)) / (max(x) - min(x)))
}
aba[1:7] <- data.frame(lapply(aba[1:7], normalize))
summary(aba$shucked_weight)
#exercise 2
k.list <- c(2,3,4,5)
wcss.list <- c()
for (k in k.list) {
abalone.km <- kmeans(dataset[,2:4], centers = k)
wcss <- abalone.km$tot.withinss
wcss.list <- c(wcss.list,wcss)
#get and plot clustering output
assigned.clusters <- as.factor(abalone.km$cluster)
}
#lab 3
###################
##### Abalone #####
###################
#abalone_dataset_prep
# read dataset
abalone <- read.csv("C:/Users/Jennifer Canfield/Desktop/Data Analytics/Lab3/abalone_dataset.csv")
#lab 3
###################
##### Abalone #####
###################
#abalone_dataset_prep
# read dataset
setwd("C:/Users/mlitc/Documents/Data_Analytics/lab3/")
abalone_dataset <- read_csv("abalone_dataset.csv")
dataset <- abalone
## add new column age.group with 3 values based on the number of rings
dataset$age.group <- cut(dataset$rings, br=c(0,8,11,35), labels = c("young", 'adult', 'old'))
## alternative way of setting age.group
dataset$age.group[dataset$rings<=8] <- "young"
dataset$age.group[dataset$rings>8 & dataset$rings<=11] <- "adult"
dataset$age.group[dataset$rings>11 & dataset$rings<=35] <- "old"
##Exercise 1
#knn model 1
sqrt(4176) #use to find k
knn.predicted <- knn(train =dataset[,2:4], test = dataset[,2:4], cl = dataset$age.group, k=65)
contingency.table <- table(knn.predicted, dataset$age.group, dnn=list('predicted','actual'))
contingency.table
#calculate classification accuracy
sum(diag(contingency.table))/length(dataset$age.group)
k.list <- c(59,61,63,65,67,69,71)
#empty list for accuracy
accuracy.list <- c()
#train & predict model for each k, compute accuracy and append it to list
for (k in k.list) {
knn.predicted <- knn(train =dataset[,2:4], test = dataset[,2:4], cl = dataset$age.group, k=k)
contigency.table <-table(knn.predicted, dataset$age.group, dnn=list('predicted','actual'))
accuracy <- sum(diag(contigency.table))/length(dataset$age.group)
accuracy.list <- c(accuracy.list,accuracy)
}
plot(k.list, accuracy.list, type = 'b')
#knn model 2
knn.predicted <- knn(train =dataset[,3:5], test = dataset[,3:5], cl = dataset$age.group, k=65)
contingency.table <- table(knn.predicted, dataset$age.group, dnn=list('predicted','actual'))
contingency.table
#calculate classification accuracy
sum(diag(contigency.table))/length(dataset$age.group)
k.list <- c(59,61,63,65,67,69,71)
#empty list for accuracy
accuracy.list <- c()
#train & predict model for each k, compute accuracy and append it to list
for (k in k.list) {
knn.predicted <- knn(train =dataset[,3:5], test = dataset[,3:5], cl = dataset$age.group, k=k)
contigency.table <-table(knn.predicted, dataset$age.group, dnn=list('predicted','actual'))
accuracy <- sum(diag(contigency.table))/length(dataset$age.group)
accuracy.list <- c(accuracy.list,accuracy)
}
plot(k.list, accuracy.list, type = 'b')
#exercise 2
k.list <- c(2,3,4,5)
wcss.list <- c()
for (k in k.list) {
abalone.km <- kmeans(dataset[,2:4], centers = k)
wcss <- abalone.km$tot.withinss
wcss.list <- c(wcss.list,wcss)
#get and plot clustering output
assigned.clusters <- as.factor(abalone.km$cluster)
}
plot(k.list,wcss.list,typbe = 'b')
# Load required libraries
library(tidyverse)
library(class)
library(caret)
library(cluster)
library(factoextra)
library(GGally)
library(psych)
library(dendextend)
# Load required libraries
library(class)
library(caret)
library(cluster)
library(colorspace)
#----------------------------------------------------------
# 1. Data Preparation
#----------------------------------------------------------
setwd("C:/Users/mlitc/Documents/Data_Analytics/lab3/")
abalone <- read_csv("abalone_dataset.csv")
# Create age group categories based on number of rings
abalone <- abalone %>%
mutate(age.group = case_when(
rings <= 8 ~ "young",
rings > 8 & rings <= 11 ~ "adult",
rings > 11 ~ "old"
))
abalone$age.group <- as.factor(abalone$age.group)
# Preview data
glimpse(abalone)
set.seed(123)
n <- nrow(abalone)
train.idx <- sample(1:n, 0.7 * n)
train.set <- abalone[train.idx, ]
test.set  <- abalone[-train.idx, ]
# --- Model 1: Columns 2–4 ---
k.init <- round(sqrt(n))
knn1 <- knn(train = train.set[, 2:4],
test  = test.set[, 2:4],
cl    = train.set$age.group,
k     = k.init)
tab1 <- table(Predicted = knn1, Actual = test.set$age.group)
acc1 <- sum(diag(tab1)) / nrow(test.set)
cat("\nModel 1 Accuracy:", round(acc1, 4), "\n")
print(tab1)
# --- Model 2: Columns 5–8 ---
knn2 <- knn(train = train.set[, 5:8],
test  = test.set[, 5:8],
cl    = train.set$age.group,
k     = k.init)
tab2 <- table(Predicted = knn2, Actual = test.set$age.group)
acc2 <- sum(diag(tab2)) / nrow(test.set)
cat("\nModel 2 Accuracy:", round(acc2, 4), "\n")
print(tab2)
# Choose best model based on accuracy
best.features <- ifelse(acc2 > acc1, 5:8, 2:4)
k.values <- seq(45, 205, by = 20)
acc.results <- numeric(length(k.values))
for (i in seq_along(k.values)) {
k.temp <- k.values[i]
pred <- knn(train = train.set[, best.features],
test  = test.set[, best.features],
cl    = train.set$age.group,
k     = k.temp)
cm <- table(pred, test.set$age.group)
acc.results[i] <- sum(diag(cm)) / nrow(test.set)
}
plot(k.values, acc.results, type = "b",
col = "steelblue", pch = 19,
main = "kNN Accuracy vs k",
xlab = "k", ylab = "Accuracy")
best.k <- k.values[which.max(acc.results)]
cat("\nOptimal k =", best.k, "with accuracy =", max(acc.results), "\n")
# Visualize relationships among features
ggpairs(abalone[, c(5:8, which(names(abalone) == "age.group"))],
aes(color = age.group))
# Load required libraries
library("ggplot2")
library(class)
library(caret)
library(cluster)
#----------------------------------------------------------
# 1. Data Preparation
#----------------------------------------------------------
setwd("C:/Users/mlitc/Documents/Data_Analytics/lab3/")
abalone <- read_csv("abalone_dataset.csv")
# Create age group categories based on number of rings
abalone <- abalone %>%
mutate(age.group = case_when(
rings <= 8 ~ "young",
rings > 8 & rings <= 11 ~ "adult",
rings > 11 ~ "old"
))
abalone$age.group <- as.factor(abalone$age.group)
# Preview data
glimpse(abalone)
set.seed(123)
n <- nrow(abalone)
train.idx <- sample(1:n, 0.7 * n)
train.set <- abalone[train.idx, ]
test.set  <- abalone[-train.idx, ]
# --- Model 1: Columns 2–4 ---
k.init <- round(sqrt(n))
knn1 <- knn(train = train.set[, 2:4],
test  = test.set[, 2:4],
cl    = train.set$age.group,
k     = k.init)
tab1 <- table(Predicted = knn1, Actual = test.set$age.group)
acc1 <- sum(diag(tab1)) / nrow(test.set)
cat("\nModel 1 Accuracy:", round(acc1, 4), "\n")
print(tab1)
# --- Model 2: Columns 5–8 ---
knn2 <- knn(train = train.set[, 5:8],
test  = test.set[, 5:8],
cl    = train.set$age.group,
k     = k.init)
tab2 <- table(Predicted = knn2, Actual = test.set$age.group)
acc2 <- sum(diag(tab2)) / nrow(test.set)
cat("\nModel 2 Accuracy:", round(acc2, 4), "\n")
print(tab2)
# Choose best model based on accuracy
best.features <- ifelse(acc2 > acc1, 5:8, 2:4)
k.values <- seq(45, 205, by = 20)
acc.results <- numeric(length(k.values))
for (i in seq_along(k.values)) {
k.temp <- k.values[i]
pred <- knn(train = train.set[, best.features],
test  = test.set[, best.features],
cl    = train.set$age.group,
k     = k.temp)
cm <- table(pred, test.set$age.group)
acc.results[i] <- sum(diag(cm)) / nrow(test.set)
}
plot(k.values, acc.results, type = "b",
col = "steelblue", pch = 19,
main = "kNN Accuracy vs k",
xlab = "k", ylab = "Accuracy")
best.k <- k.values[which.max(acc.results)]
cat("\nOptimal k =", best.k, "with accuracy =", max(acc.results), "\n")
# Visualize relationships among features
ggpairs(abalone[, c(5:8, which(names(abalone) == "age.group"))],
aes(color = age.group))
# Determine optimal K using Elbow and Silhouette
k.list <- 2:6
wcss <- c()
sil.means <- c()
for (k in k.list) {
km <- kmeans(abalone[, best.features], centers = k)
wcss <- c(wcss, km$tot.withinss)
sil <- silhouette(km$cluster, dist(abalone[, best.features]))
sil.means <- c(sil.means, mean(sil[, 3]))
}
# Plot metrics
par(mfrow = c(1, 2))
plot(k.list, wcss, type = "b", pch = 19, col = "darkorange",
main = "Elbow Method", xlab = "k", ylab = "WCSS")
plot(k.list, sil.means, type = "b", pch = 19, col = "forestgreen",
main = "Silhouette Method", xlab = "k", ylab = "Mean Silhouette Width")
best.kmeans.k <- k.list[which.max(sil.means)]
cat("\nBest K-Means k =", best.kmeans.k, "\n")
# Final K-Means model
final.km <- kmeans(abalone[, best.features], centers = best.kmeans.k)
fviz_cluster(final.km, data = abalone[, best.features])
sil <- silhouette(final.km$cluster, dist(abalone[, best.features]))
fviz_silhouette(sil)
pam.sil <- c()
pam.diss <- c()
for (k in k.list) {
pam.model <- pam(abalone[, best.features], k)
pam.diss <- c(pam.diss, pam.model$objective[1])
sil <- silhouette(pam.model$clustering, dist(abalone[, best.features]))
pam.sil <- c(pam.sil, mean(sil[, 3]))
}
# Load required libraries
library(class)
library(caret)
library(cluster)
#----------------------------------------------------------
# 1. Data Preparation
#----------------------------------------------------------
setwd("C:/Users/mlitc/Documents/Data_Analytics/lab3/")
abalone <- read_csv("abalone_dataset.csv")
# Load required libraries
library(class)
library(caret)
library(cluster)
library(readr)
#----------------------------------------------------------
# 1. Data Preparation
#----------------------------------------------------------
setwd("C:/Users/mlitc/Documents/Data_Analytics/lab3/")
abalone <- read_csv("abalone_dataset.csv")
# Create age group categories based on number of rings
abalone <- abalone %>%
mutate(age.group = case_when(
rings <= 8 ~ "young",
rings > 8 & rings <= 11 ~ "adult",
rings > 11 ~ "old"
))
abalone$age.group <- as.factor(abalone$age.group)
# Load required libraries
library(class)
# Load required libraries
library(tidyverse)
library(class)
library(caret)
library(cluster)
library(factoextra)
library(GGally)
library(psych)
library(dendextend)
library(colorspace)
#----------------------------------------------------------
# 1. Data Preparation
#----------------------------------------------------------
setwd("C:/Users/mlitc/Documents/Data_Analytics/lab3/")
abalone <- read_csv("abalone_dataset.csv")
# Load required libraries
library(tidyverse)
library(class)
library(caret)
library(cluster)
library(factoextra)
#----------------------------------------------------------
# 1. Data Preparation
#----------------------------------------------------------
setwd("C:/Users/mlitc/Documents/Data_Analytics/lab3/")
abalone <- read_csv("abalone_dataset.csv")
# --- Libraries --------------------------------------------------------
library(tidyverse)    # dplyr, ggplot2, readr
library(class)        # knn
library(cluster)      # pam, silhouette
library(factoextra)   # fviz_silhouette, fviz_cluster
# --- Load data (adjust path if needed) --------------------------------
file_path <- "abalone_dataset.csv"            # put file in working dir or change path
abalone <- readr::read_csv(file_path, show_col_types = FALSE)
# quick view of columns (helps avoid name typos)
cat("Columns found in dataset:\n")
print(colnames(abalone))
# --- Create age.group (3 classes) ------------------------------------
# Uses rings to create factor with 3 levels: young/adult/old
abalone <- abalone %>%
mutate(age.group = cut(rings,
breaks = c(-Inf, 8, 11, Inf),
labels = c("young", "adult", "old"),
right = TRUE)) %>%
mutate(age.group = factor(age.group, levels = c("young", "adult", "old")))
# --- Train/test split -------------------------------------------------
set.seed(42)
n_all <- nrow(abalone)
train_idx <- sample(seq_len(n_all), size = floor(0.70 * n_all))
train_set <- abalone[train_idx, ]
test_set  <- abalone[-train_idx, ]
# helper to ensure numeric matrix for knn
as_num_mat <- function(df) {
# keep numeric columns but coerce explicitly to numeric matrix
m <- as.matrix(sapply(df, as.numeric))
return(m)
}
# --- kNN models (two different feature subsets) -----------------------
# NOTE: these use the same column-index choices as your original code
subset1 <- 2:4   # model 1 features (columns 2..4)
subset2 <- 5:8   # model 2 features (columns 5..8)
# initialize k (simple heuristic)
k_init <- max(1, round(sqrt(nrow(train_set))))
# Model 1
train_x1 <- as_num_mat(train_set[, subset1])
test_x1  <- as_num_mat(test_set[, subset1])
knn1_pred <- knn(train = train_x1, test = test_x1, cl = train_set$age.group, k = k_init)
# load required libraries
library(class)      # for kNN
library(caret)      # for confusionMatrix
library(dplyr)      # for data manipulation
library(ggplot2)    # for plotting
library(cluster)    # for PAM and silhouette
library(factoextra) # for silhouette visualization
# --- read dataset ---
abalone.data <- read.csv("C:/Users/mlitc/Documents/Data_Analytics/lab3/abalone_dataset.csv")
# --- add age.group variable based on number of rings ---
abalone.data$age.group <- cut(abalone.data$rings,
br = c(0, 8, 11, 35),
labels = c("young", "adult", "old"))
# alternative explicit assignment
abalone.data$age.group[abalone.data$rings <= 8] <- "young"
abalone.data$age.group[abalone.data$rings > 8 & abalone.data$rings <= 11] <- "adult"
abalone.data$age.group[abalone.data$rings > 11 & abalone.data$rings <= 35] <- "old"
abalone.data$age.group <- factor(abalone.data$age.group,
levels = c("young", "adult", "old"))
set.seed(123)
n <- nrow(abalone.data)
train.index <- sample(1:n, size = 0.7 * n)
train.data <- abalone.data[train.index, ]
test.data  <- abalone.data[-train.index, ]
# a quick heuristic for starting k
k.start <- round(sqrt(n))
### --- Model 1: first subset of features (e.g. columns 2:4) ---
knn.pred.1 <- knn(train = train.data[, 2:4],
test  = test.data[, 2:4],
cl    = train.data$age.group,
k     = k.start)
table.1 <- table(predicted = knn.pred.1, actual = test.data$age.group)
cat("\n=== Model 1 Contingency Table ===\n")
print(table.1)
acc.1 <- sum(diag(table.1)) / length(test.data$age.group)
cat("Model 1 Accuracy =", round(acc.1, 4), "\n")
### --- Model 2: second subset of features (e.g. columns 5:8) ---
knn.pred.2 <- knn(train = train.data[, 5:8],
test  = test.data[, 5:8],
cl    = train.data$age.group,
k     = k.start)
table.2 <- table(predicted = knn.pred.2, actual = test.data$age.group)
cat("\n=== Model 2 Contingency Table ===\n")
print(table.2)
acc.2 <- sum(diag(table.2)) / length(test.data$age.group)
cat("Model 2 Accuracy =", round(acc.2, 4), "\n")
### --- pick better-performing model ---
if (acc.1 >= acc.2) {
best.features <- 2:4
best.model <- "Model 1"
best.acc <- acc.1
} else {
best.features <- 5:8
best.model <- "Model 2"
best.acc <- acc.2
}
cat("\nBetter performing model = ", best.model,
" with accuracy =", round(best.acc, 4), "\n")
k.values <- seq(5, 105, by = 10)
accuracy.values <- numeric(length(k.values))
for (i in seq_along(k.values)) {
k <- k.values[i]
pred <- knn(train = train.data[, best.features],
test  = test.data[, best.features],
cl    = train.data$age.group,
k     = k)
cm <- table(pred, test.data$age.group)
accuracy.values[i] <- sum(diag(cm)) / length(test.data$age.group)
}
plot(k.values, accuracy.values, type = "b", pch = 19, col = "steelblue",
main = paste("Accuracy vs k (", best.model, " features )"),
xlab = "k (neighbors)", ylab = "Classification Accuracy")
best.k <- k.values[which.max(accuracy.values)]
cat("\nOptimal k =", best.k,
" with accuracy =", round(max(accuracy.values), 4), "\n")
# use best feature subset found above
cluster.data <- scale(abalone.data[, best.features])
# visualize data (pairwise)
pairs(cluster.data, main = "Selected Features (Scaled)")
k.list <- 2:6
wcss <- numeric(length(k.list))
sil.mean.km <- numeric(length(k.list))
for (i in seq_along(k.list)) {
k <- k.list[i]
km <- kmeans(cluster.data, centers = k, nstart = 25)
wcss[i] <- km$tot.withinss
sil <- silhouette(km$cluster, dist(cluster.data))
sil.mean.km[i] <- mean(sil[, 3])
}
# plot WCSS and Silhouette
par(mfrow = c(1, 2))
plot(k.list, wcss, type = "b", pch = 19, col = "darkorange",
main = "K-Means Elbow (WCSS)",
xlab = "K", ylab = "Within-Cluster SS")
plot(k.list, sil.mean.km, type = "b", pch = 19, col = "forestgreen",
main = "K-Means Average Silhouette",
xlab = "K", ylab = "Avg Silhouette Width")
best.k.kmeans <- k.list[which.max(sil.mean.km)]
cat("\nBest K for K-Means =", best.k.kmeans, "\n")
final.kmeans <- kmeans(cluster.data, centers = best.k.kmeans, nstart = 25)
fviz_cluster(final.kmeans, data = cluster.data,
main = paste("K-Means ( K =", best.k.kmeans, " )"))
# silhouette plot
sil.kmeans <- silhouette(final.kmeans$cluster, dist(cluster.data))
fviz_silhouette(sil.kmeans)
sil.mean.pam <- numeric(length(k.list))
sum.diss <- numeric(length(k.list))
for (i in seq_along(k.list)) {
k <- k.list[i]
pam.model <- pam(cluster.data, k)
sum.diss[i] <- pam.model$objective[1]
sil <- silhouette(pam.model$clustering, dist(cluster.data))
sil.mean.pam[i] <- mean(sil[, 3])
}
# plot dissimilarity and silhouette
par(mfrow = c(1, 2))
plot(k.list, sum.diss, type = "b", pch = 19, col = "tomato",
main = "PAM – Sum of Dissimilarities",
xlab = "K", ylab = "Sum Dissimilarity")
plot(k.list, sil.mean.pam, type = "b", pch = 19, col = "royalblue",
main = "PAM – Average Silhouette Width",
xlab = "K", ylab = "Avg Silhouette Width")
best.k.pam <- k.list[which.max(sil.mean.pam)]
cat("\nBest K for PAM =", best.k.pam, "\n")
final.pam <- pam(cluster.data, best.k.pam)
fviz_cluster(final.pam, geom = "point", ellipse.type = "convex",
main = paste("PAM ( K =", best.k.pam, " )"))
# load required libraries
library(class)      # for kNN
library(caret)      # for confusionMatrix
library(dplyr)      # for data manipulation
library(ggplot2)    # for plotting
library(cluster)    # for PAM and silhouette
library(factoextra) # for silhouette visualization
